{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# TextLoader를 사용하여 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/keywords_examples.txt\")\n",
    "\n",
    "# 문서를 로드합니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 300이고 청크 간 중복은 없습니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 로드된 문서를 분할합니다.\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(split_docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 벡터 형태로 변환하여 컴퓨터가 의미를 파악할 수 있도록 하는 기술입니다.\n",
      "예시: \"사과\"를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할한 최소 처리 단위로, 일반적으로 단어 또는 형태소를 의미합니다.\n",
      "예시: \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어 처리, 형태소 분석\n",
      "=========================================================\n",
      "Named Entity Recognition (NER)\n",
      "정의: NER은 문장 내 인물명, 지명, 기관명 등 특정 개체를 인식하고 태깅하는 작업입니다.\n",
      "예시: \"서울에서 김철수가 스타벅스를 방문했다\"에서 \"서울(지명)\", \"김철수(인명)\", \"스타벅스(기관명)\" 추출.\n",
      "연관키워드: 정보추출, 개체링크, 정보검색\n",
      "\n",
      "Machine Translation\n",
      "정의: 기계 번역은 한 언어의 텍스트를 다른 언어로 자동 변환하는 기술입니다.\n",
      "예시: \"Hello\"를 \"안녕하세요\"로 번역.\n",
      "연관키워드: 번역 모델, Seq2Seq, Transformer\n",
      "=========================================================\n",
      "Language Modeling\n",
      "정의: 언어 모델링은 단어나 문장 시퀀스에 대해 확률 분포를 학습하는 과정입니다.\n",
      "예시: 주어진 단어열 다음에 올 단어의 확률 예측.\n",
      "연관키워드: NLP, 확률 모델, 시퀀스 예측\n",
      "\n",
      "Coherence\n",
      "정의: 응집성은 문장이나 문단 내 문맥적 일관성을 가리킵니다.\n",
      "예시: \"나는 밥을 먹고, 학교에 갔다\"는 논리적 흐름이 있음.\n",
      "연관키워드: 문맥 일관성, 의미 연결성\n",
      "=========================================================\n",
      "N-gram\n",
      "정의: N-그램은 연속된 N개의 토큰 시퀀스로 언어모델에서 다음 단어 예측 확률 계산에 활용됩니다.\n",
      "예시: \"나는 학교에 간다\"에서 바이그램: (\"나는\", \"학교에\"), (\"학교에\", \"간다\")\n",
      "연관키워드: 통계적 언어모델, 확률 모델링\n",
      "\n",
      "Word2Vec\n",
      "정의: Word2Vec은 단어를 벡터로 표현하기 위한 신경망 기반 임베딩 기법입니다.\n",
      "예시: 유사 의미 단어는 벡터 공간에서 가깝게 위치.\n",
      "연관키워드: 임베딩, 딥러닝, 분포 가설\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
